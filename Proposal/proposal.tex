\documentclass{article}

\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[margin=1.2in]{geometry}
\lhead{K. Dercksen, M. K\"onings}
\chead{4215966 - 4221400}
\rhead{Datamining Project Proposal}
\cfoot{\thepage}
\pagestyle{fancy}

\hypersetup{colorlinks=true, urlcolor=blue}

\begin{document}

We will be implementing OC1, a system for induction of oblique decision trees. OC1 works by finding oblique splits (hyperplanes) for each node of a decision tree. These trees are especially meant for data with numeric attributes.

\section*{Algorithm}
Basic decision tree induction algorithms look like the following (consider training records $E$ and attribute set $F$):
\begin{figure}[h!]
\begin{lstlisting}[frame=single]
build_tree(E, F):
  if stopping_condition is true:
    leaf = new node
    leaf:label = classify(E)
    return leaf
  root = new node
  root:test_condition = best_split(E, F)
  let V = {v | v is a possible outcome of root:test_condition}
  for each v in V:
    E_v = {e | root:test_condition(e) = v and e in E}
    child = build_tree(E_v, F)
    add child as descendent of root and label edge (root -> child) as v
  return root
\end{lstlisting}
\caption{Taken from Introduction to Datamining by Tan, Steinback, Kumar}
\end{figure}

OC1 is special because of its splitting criterion; finding
\begin{align*}
\sum\limits_{i=1}^{d}a_i x_i + a_{d+1} > 0
\end{align*}
at every node, where $a_1, \ldots, a_{d+1}$ are real-valued coefficients. Since these tests are equivalent to hyperplanes at an oblique orientation to the axes, this class of decision trees is called \emph{oblique}.

\section*{Dataset}
We're thinking of using the following two datasets to train our implementation. These are often used datasets (the iris set is even part of the sklearn package for Python), so we think they'll be great to test on.
\begin{enumerate}
\item Iris flower data set\cite{Bache+Lichman:2013}.
\item ISOLET data set\cite{Bache+Lichman:2013}.
\end{enumerate}

\section*{Literature}
Literature we found so far that'll guide our implementation (at least we think so):

\begin{enumerate}
\item A System for Induction of Oblique Decision Trees \cite{KSM:1994}.
\end{enumerate}

\bibliographystyle{plain}
\bibliography{bib}

\end{document}
